#!/usr/bin/env python3
"""
Loop Selector - Intelligent loop count recommendation

Analyzes model performance and recommends optimal loop count
based on FPS measurement and target duration.

Usage:
    loop-selector model.dxnn
    loop-selector model.dxnn --target-duration 5.0
    loop-selector model.dxnn --format json
    loop-selector model.dxnn --update-yaml config.yaml
"""

import argparse
import json
import os
import re
import subprocess
import sys
import time
import yaml
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional


# ============================================================================
# Core Loop Selection Logic (Integrated)
# ============================================================================

class LoopSelector:
    """
    Intelligent loop selector with FPS measurement and statistical validation
    
    This is a standalone implementation that doesn't depend on external modules.
    """
    
    def __init__(self, run_model_cmd: str, logger=None, quiet=False):
        """
        Initialize loop selector
        
        Args:
            run_model_cmd: Path to run_model executable
            logger: Optional logger function (defaults to print to stderr)
            quiet: If True, suppress all output
        """
        self.run_model_cmd = run_model_cmd
        self.quiet = quiet
        if quiet:
            self.logger = lambda x: None  # No-op
        elif logger:
            self.logger = logger
        else:
            # Default: print to stderr to keep stdout clean for JSON
            self.logger = lambda x: print(x, file=sys.stderr)
    
    def log(self, message: str):
        """Log message"""
        if self.logger and not self.quiet:
            self.logger(message)
    
    def calculate_initial_loops(self, model_path: str) -> int:
        """
        Calculate initial test loops based on model size
        
        Args:
            model_path: Path to model file
            
        Returns:
            Number of initial test loops
        """
        try:
            model_size_mb = os.path.getsize(model_path) / (1024 * 1024)
        except:
            return 5
        
        if model_size_mb < 5:
            return 20
        elif model_size_mb < 20:
            return 10
        elif model_size_mb < 100:
            return 5
        else:
            return 3
    
    def calculate_warmup_runs(self, model_path: str) -> int:
        """
        Calculate appropriate warmup runs based on model size
        
        Args:
            model_path: Path to model file
            
        Returns:
            Number of warmup runs
        """
        try:
            model_size_mb = os.path.getsize(model_path) / (1024 * 1024)
        except:
            return 3
        
        if model_size_mb < 10:
            return 2
        elif model_size_mb < 50:
            return 3
        else:
            return 5
    
    def measure_fps_single(self, model_path: str, loops: int, warmup: int = 2) -> Dict:
        """
        Run a single FPS measurement
        
        Args:
            model_path: Path to model file
            loops: Number of inference loops
            warmup: Number of warmup runs
            
        Returns:
            Dictionary with measurement results
        """
        try:
            cmd = [
                self.run_model_cmd,
                '-m', model_path,
                '-l', str(loops),
                '-w', str(warmup),
                '--use-ort',
                '-v'
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300
            )
            elapsed = time.time() - start_time
            
            if result.returncode != 0:
                return {
                    'success': False,
                    'error': f"Command failed with code {result.returncode}",
                    'elapsed': elapsed
                }
            
            # Parse FPS from output
            fps_match = re.search(r'FPS\s*:\s*(\d+\.?\d*)', result.stdout)
            if not fps_match:
                return {
                    'success': False,
                    'error': 'Could not parse FPS from output',
                    'elapsed': elapsed
                }
            
            fps = float(fps_match.group(1))
            
            # Parse optional metrics
            latency_match = re.search(r'Latency Average\s*:\s*(\d+\.?\d*)\s*ms', result.stdout)
            npu_match = re.search(r'NPU Processing Time Average\s*:\s*(\d+\.?\d*)\s*ms', result.stdout)
            
            return {
                'success': True,
                'fps': fps,
                'latency': float(latency_match.group(1)) if latency_match else None,
                'npu_time': float(npu_match.group(1)) if npu_match else None,
                'elapsed': elapsed
            }
            
        except subprocess.TimeoutExpired:
            return {'success': False, 'error': 'Timeout after 300s', 'elapsed': 300.0}
        except Exception as e:
            return {'success': False, 'error': str(e), 'elapsed': 0.0}
    
    def measure_fps_with_confidence(self, 
                                    model_path: str, 
                                    loops: int, 
                                    warmup: int = 2,
                                    samples: int = 3) -> Dict:
        """
        Measure FPS multiple times and calculate confidence interval
        
        Args:
            model_path: Path to model file
            loops: Number of inference loops per sample
            warmup: Number of warmup runs
            samples: Number of measurement samples
            
        Returns:
            Dictionary with measurement results and statistics
        """
        fps_measurements = []
        latency_measurements = []
        npu_time_measurements = []
        total_time = 0.0
        
        for i in range(samples):
            result = self.measure_fps_single(model_path, loops, warmup)
            total_time += result.get('elapsed', 0)
            
            if result['success'] and 'fps' in result:
                fps_measurements.append(result['fps'])
                self.log(f"    Sample {i+1}/{samples}: {result['fps']:.2f} FPS")
                
                if result.get('latency') is not None:
                    latency_measurements.append(result['latency'])
                if result.get('npu_time') is not None:
                    npu_time_measurements.append(result['npu_time'])
            else:
                self.log(f"    Sample {i+1}/{samples}: Failed - {result.get('error', 'Unknown')}")
        
        if len(fps_measurements) < 2:
            return {
                'success': False,
                'error': 'Insufficient measurements (need at least 2)',
                'elapsed_time': total_time
            }
        
        # Calculate statistics
        mean_fps = np.mean(fps_measurements)
        std_fps = np.std(fps_measurements, ddof=1)
        cv = (std_fps / mean_fps * 100) if mean_fps > 0 else 100
        
        # 95% confidence interval
        margin = 1.96 * std_fps / np.sqrt(len(fps_measurements))
        confidence_95 = (mean_fps - margin, mean_fps + margin)
        
        is_stable = cv < 5.0
        
        avg_latency = np.mean(latency_measurements) if latency_measurements else None
        avg_npu_time = np.mean(npu_time_measurements) if npu_time_measurements else None
        
        return {
            'success': True,
            'mean_fps': mean_fps,
            'std_fps': std_fps,
            'cv': cv,
            'measurements': fps_measurements,
            'is_stable': is_stable,
            'confidence_95': confidence_95,
            'elapsed_time': total_time,
            'latency': avg_latency,
            'npu_time': avg_npu_time
        }
    
    def smart_initial_measurement(self, model_path: str, samples: int = 3) -> Dict:
        """
        Intelligent initial FPS measurement with stability compensation
        
        Args:
            model_path: Path to model file
            samples: Number of measurement samples
            
        Returns:
            Dictionary with comprehensive measurement results
        """
        self.log("  [Phase 1] Intelligent Initial Measurement")
        
        initial_loops = self.calculate_initial_loops(model_path)
        self.log(f"    Initial loops: {initial_loops} (based on model size)")
        
        warmup = self.calculate_warmup_runs(model_path)
        self.log(f"    Warmup runs: {warmup}")
        
        result = self.measure_fps_with_confidence(
            model_path, 
            loops=initial_loops,
            warmup=warmup,
            samples=samples
        )
        
        if not result['success']:
            return {
                'success': False,
                'error': result['error'],
                'elapsed': result['elapsed_time']
            }
        
        # Stability compensation
        compensation_performed = False
        max_compensation_attempts = 3
        attempt = 0
        
        while result['cv'] > 5.0 and attempt < max_compensation_attempts:
            attempt += 1
            self.log(f"    ‚ö†Ô∏è  Measurement unstable (CV: {result['cv']:.2f}%), performing compensation (attempt {attempt}/{max_compensation_attempts})...")
            
            compensated_loops = initial_loops * (2 ** attempt)
            compensated_samples = max(samples * (2 ** attempt), 5)
            compensated_warmup = warmup + attempt
            
            self.log(f"    Compensation: loops {initial_loops * (2 ** (attempt-1))}‚Üí{compensated_loops}, samples {max(samples * (2 ** (attempt-1)), 5)}‚Üí{compensated_samples}")
            
            compensated_result = self.measure_fps_with_confidence(
                model_path,
                loops=compensated_loops,
                warmup=compensated_warmup,
                samples=compensated_samples
            )
            
            if compensated_result['success']:
                if compensated_result['cv'] < result['cv']:
                    self.log(f"    ‚úÖ Compensation successful: CV {result['cv']:.2f}% ‚Üí {compensated_result['cv']:.2f}%")
                    result = compensated_result
                    compensation_performed = True
                else:
                    self.log(f"    ‚ö†Ô∏è  Compensation did not improve CV, stopping")
                    break
            else:
                self.log(f"    ‚ö†Ô∏è  Compensation failed, stopping")
                break
        
        self.log(f"    Mean FPS: {result['mean_fps']:.2f} (¬±{result['std_fps']:.2f})")
        self.log(f"    CV: {result['cv']:.2f}%")
        self.log(f"    Stable: {'Yes' if result['is_stable'] else 'No'}")
        if compensation_performed:
            self.log(f"    Compensation: Performed")
        if result.get('latency'):
            self.log(f"    Latency: {result['latency']:.2f}ms")
        if result.get('npu_time'):
            self.log(f"    NPU Time: {result['npu_time']:.2f}ms")
        
        return {
            'success': True,
            'fps': result['mean_fps'],
            'std_fps': result['std_fps'],
            'cv': result['cv'],
            'initial_loops': initial_loops,
            'is_stable': result['is_stable'],
            'measurements': result['measurements'],
            'confidence_95': result['confidence_95'],
            'elapsed': result['elapsed_time'],
            'latency': result.get('latency'),
            'npu_time': result.get('npu_time'),
            'compensation_performed': compensation_performed
        }
    
    def round_to_nice_number(self, value: int) -> int:
        """Round to human-friendly numbers"""
        # Always ensure at least 1 loop
        if value < 1:
            return 1
        elif value < 50:
            return value
        elif value < 100:
            return round(value / 10) * 10
        elif value < 1000:
            return round(value / 50) * 50
        else:
            return round(value / 100) * 100
    
    def select_loops(self,
                    model_path: str,
                    target_duration: float = 3.0,
                    samples: int = 3) -> Dict:
        """
        Complete loop selection algorithm
        
        Args:
            model_path: Path to model file
            target_duration: Target measurement duration in seconds
            samples: Number of measurement samples
            
        Returns:
            Dictionary with selected loops and metadata
        """
        self.log("--- Intelligent Loop Selection ---")
        
        initial_result = self.smart_initial_measurement(model_path, samples=samples)
        
        if not initial_result['success']:
            fallback_loops = 100
            self.log(f"  ‚ö†Ô∏è  Initial measurement failed: {initial_result.get('error')}")
            self.log(f"  Falling back to default: {fallback_loops} loops")
            return {
                'success': False,
                'loops': fallback_loops,
                'reason': 'Initial measurement failed, using fallback',
                'metadata': {'error': initial_result.get('error')}
            }
        
        mean_fps = initial_result['fps']
        calculated_loops = int(mean_fps * target_duration)
        
        # Ensure at least 1 loop (critical for very low FPS models)
        if calculated_loops < 1:
            calculated_loops = 1
        
        final_loops = self.round_to_nice_number(calculated_loops)
        
        reason = (f"Target: {target_duration:.1f}s, "
                  f"FPS: {mean_fps:.1f}, "
                  f"Calculated: {calculated_loops} ‚Üí "
                  f"Final: {final_loops}")
        
        metadata = {
            'initial_fps': initial_result['fps'],
            'initial_cv': initial_result['cv'],
            'target_duration': target_duration,
            'is_stable': initial_result['is_stable'],
            'confidence_95': initial_result['confidence_95'],
            'elapsed_time': initial_result['elapsed'],
            'initial_latency': initial_result.get('latency'),
            'initial_npu_time': initial_result.get('npu_time'),
            'compensation_performed': initial_result.get('compensation_performed', False)
        }
        
        self.log(f"\n    Measurement time: {initial_result['elapsed']:.1f}s")
        self.log(f"\n  ‚úì Target duration: {target_duration}s")
        self.log(f"  ‚úì Selected loops: {final_loops}")

        return {
            'success': True,
            'loops': final_loops,
            'reason': reason,
            'metadata': metadata
        }


def print_detailed_explanation(result, args):
    """Print detailed calculation explanation"""
    meta = result['metadata']
    
    print()
    print("‚îÅ" * 70)
    print(" Loop Selector - Calculation Details")
    print("‚îÅ" * 70)
    print()
    print("üìä Model Performance Measurement:")
    print(f"   Model: {Path(args.model).name}")
    print(f"   Measured FPS: {meta['initial_fps']:.1f} (CV: {meta['initial_cv']:.2f}%)")
    if meta.get('initial_latency'):
        print(f"   Latency: {meta['initial_latency']:.2f}ms")
    if meta.get('initial_npu_time'):
        print(f"   NPU Time: {meta['initial_npu_time']:.2f}ms")
    print()
    print("üéØ Target Configuration:")
    print(f"   Target duration: {args.target_duration} seconds")
    print()
    print("üìê Loop Calculation:")
    print(f"   Formula: loops = FPS √ó target_duration")
    calculated = meta['initial_fps'] * args.target_duration
    print(f"   Calculated: {meta['initial_fps']:.1f} √ó {args.target_duration} = {calculated:.0f} loops")
    print(f"   Final (with rounding): {result['loops']} loops")
    print()
    print(f"‚úì Recommended: {result['loops']} loops")
    expected_duration = result['loops'] / meta['initial_fps']
    print(f"  Expected duration: ~{expected_duration:.1f} seconds")
    print()
    print("üí° Tips:")
    print(f"   ‚Ä¢ Faster testing: --target-duration 1.0 ‚Üí ~{int(meta['initial_fps'])} loops")
    print(f"   ‚Ä¢ Higher precision: --target-duration 5.0 ‚Üí ~{int(meta['initial_fps'] * 5)} loops")
    print()


def update_yaml_loop_count(yaml_file, loop_count):
    """Update loop_count in YAML file"""
    yaml_path = Path(yaml_file)
    
    if not yaml_path.exists():
        print(f"Error: YAML file not found: {yaml_file}", file=sys.stderr)
        return False
    
    try:
        with open(yaml_path, 'r') as f:
            config = yaml.safe_load(f)
        
        config['loop_count'] = loop_count
        
        with open(yaml_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)
        
        return True
    except Exception as e:
        print(f"Error updating YAML: {e}", file=sys.stderr)
        return False


def main():
    parser = argparse.ArgumentParser(
    description='Intelligent loop count selector for DX models',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Standard usage (3 second target)
  %(prog)s model.dxnn
  
  # Custom target duration
  %(prog)s model.dxnn --target-duration 5.0
  
  # JSON output for scripting
  %(prog)s model.dxnn --format json | jq -r '.loops'
  
  # Environment variables
  eval $(%(prog)s model.dxnn --format env)
  echo $DX_LOOP_COUNT
  
  # Detailed explanation
  %(prog)s model.dxnn --explain

  # Custom number of samples
  %(prog)s model.dxnn --samples 5
        '''
    )
    
    # Positional
    parser.add_argument('model', help='Model file path (.dxnn)')
    
    # Target configuration
    parser.add_argument('--target-duration', type=float, default=3.0,
                       help='Target measurement duration in seconds (default: 3.0)')
    
    # Output format
    parser.add_argument('--format', choices=['text', 'value', 'json', 'yaml', 'env'],
                       default='text', help='Output format (default: text)')
    parser.add_argument('--explain', action='store_true',
                       help='Show detailed calculation explanation')
    
    # YAML integration
    parser.add_argument('--update-yaml', metavar='FILE',
                       help='Update loop_count in YAML file')
    
    # Advanced
    parser.add_argument('--run-model', 
                       help='Path to run_model executable (auto-detected if not specified)')
    parser.add_argument('--samples', type=int, default=3,
                       help='Number of measurement samples (default: 3)')
    
    args = parser.parse_args()
    
    # Find run_model
    if not args.run_model:
        # Auto-detect run_model (in project root bin/)
        tool_dir = Path(__file__).parent
        bin_dir = tool_dir.parent.parent / 'bin'
        run_model_path = bin_dir / 'run_model'
        if run_model_path.exists():
            args.run_model = str(run_model_path)
        else:
            print("Error: run_model not found. Please specify with --run-model", file=sys.stderr)
            sys.exit(1)
    
    # Initialize selector (quiet mode for JSON/value output to keep stdout clean)
    quiet = args.format in ['json', 'value', 'yaml', 'env']
    selector = LoopSelector(args.run_model, quiet=quiet)
    
    # Select loops
    try:
        result = selector.select_loops(args.model, target_duration=args.target_duration, samples=args.samples)
    except Exception as e:
        print(f"‚úó Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    if not result['success']:
        print(f"‚úó Error: {result.get('reason')}", file=sys.stderr)
        sys.exit(1)
    
    # Output based on format
    if args.explain:
        print_detailed_explanation(result, args)
    elif args.format == 'text':
        meta = result['metadata']
        print(f"‚úì Recommended: {result['loops']} loops")
        print(f"  Target: {args.target_duration}s, FPS: {meta['initial_fps']:.1f}, CV: {meta['initial_cv']:.1f}%")
        if meta.get('compensation_performed'):
            print(f"  Compensation: Performed")
        if meta.get('initial_latency'):
            print(f"  Latency: {meta['initial_latency']:.2f}ms", end='')
            if meta.get('initial_npu_time'):
                print(f", NPU: {meta['initial_npu_time']:.2f}ms")
            else:
                print()
    elif args.format == 'value':
        print(result['loops'])
    elif args.format == 'json':
        meta = result['metadata']
        output = {
            'loops': result['loops'],
            'target_duration': args.target_duration,
            'measured_fps': float(meta['initial_fps']),
            'latency_ms': float(meta['initial_latency']) if meta.get('initial_latency') else None,
            'npu_time_ms': float(meta['initial_npu_time']) if meta.get('initial_npu_time') else None,
            'cv_percent': float(meta['initial_cv']),
            'is_stable': bool(meta['is_stable']),
            'compensation_performed': bool(meta.get('compensation_performed', False)),
            'reason': result['reason']
        }
        print(json.dumps(output, indent=2))
    elif args.format == 'yaml':
        print(f"loop_count: {result['loops']}")
    elif args.format == 'env':
        meta = result['metadata']
        print(f"export DX_LOOP_COUNT={result['loops']}")
        print(f"export DX_TARGET_DURATION={args.target_duration}")
        print(f"export DX_MEASURED_FPS={meta['initial_fps']:.1f}")
    
    # Update YAML if requested
    if args.update_yaml:
        if update_yaml_loop_count(args.update_yaml, result['loops']):
            print(f"‚úì Updated {args.update_yaml}: loop_count = {result['loops']}")
        else:
            sys.exit(1)


if __name__ == '__main__':
    main()
