#
# Copyright (C) 2018- DEEPX Ltd.
# All rights reserved.
#
# This software is the property of DEEPX and is provided exclusively to customers 
# who are supplied with DEEPX NPU (Neural Processing Unit). 
# Unauthorized sharing or usage is strictly prohibited by law.
# 
# This file uses ONNX Runtime (MIT License) - Copyright (c) Microsoft Corporation.
#

import numpy as np
import onnxruntime as ort
import os
import argparse

def get_numpy_dtype(onnx_type_str):
    """ONNX íƒ€ì… ë¬¸ìì—´ì„ NumPy ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    type_map = {
        'tensor(float)': np.float32,
        'tensor(float32)': np.float32,
        'tensor(float16)': np.float16,
        'tensor(double)': np.float64,
        'tensor(int8)': np.int8,
        'tensor(uint8)': np.uint8,
        'tensor(int16)': np.int16,
        'tensor(uint16)': np.uint16,
        'tensor(int32)': np.int32,
        'tensor(uint32)': np.uint32,
        'tensor(int64)': np.int64,
        'tensor(uint64)': np.uint64,
        'tensor(bool)': np.bool_,
    }
    return type_map.get(onnx_type_str)

def run_multi_input_inference(onnx_path, input_path, output_path, batch_size=1):
    """
    ONNX ëª¨ë¸ì˜ ë‹¤ì¤‘ ì…ë ¥ì„ ìë™ ë¶„ì„í•˜ê³ , ë‹¨ì¼ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì„ ë¶„í• í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # 1. ONNX ëŸ°íƒ€ì„ ì„¸ì…˜ ìƒì„±
    try:
        session = ort.InferenceSession(onnx_path)
        print(f"âœ… ONNX ëª¨ë¸ '{os.path.basename(onnx_path)}'ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ONNX ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return

    # 2. ëª¨ë¸ì˜ ëª¨ë“  ì…ë ¥ í…ì„œ ì •ë³´ ìë™ ë¶„ì„
    inputs_meta = session.get_inputs()
    model_inputs = []
    print("\nâ„¹ï¸ ëª¨ë¸ ì…ë ¥ ìë™ ë¶„ì„ ê²°ê³¼:")
    for i, meta in enumerate(inputs_meta):
        # shapeì˜ ë™ì  ì°¨ì›(None, -1)ì„ ì‚¬ìš©ìê°€ ì§€ì •í•œ batch_sizeë¡œ ëŒ€ì²´
        shape = [dim if isinstance(dim, int) and dim > 0 else batch_size for dim in meta.shape]
        dtype = get_numpy_dtype(meta.type)
        if dtype is None:
            print(f"âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ONNX íƒ€ì…ì…ë‹ˆë‹¤: {meta.type}")
            return
            
        # ê° ì…ë ¥ì— í•„ìš”í•œ ë°”ì´íŠ¸ í¬ê¸° ê³„ì‚°
        size_in_bytes = np.prod(shape) * np.dtype(dtype).itemsize
        
        model_inputs.append({
            'name': meta.name,
            'shape': shape,
            'dtype': dtype,
            'size_bytes': int(size_in_bytes)
        })
        print(f"  - ì…ë ¥ #{i+1}: ì´ë¦„='{meta.name}', Shape={shape}, Type={dtype.__name__}")

    # 3. ë‹¨ì¼ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì½ê¸° ë° ë¶„í• 
    try:
        full_input_bytes = open(input_path, 'rb').read()
        print(f"\nâœ… ì…ë ¥ íŒŒì¼ '{os.path.basename(input_path)}' ({len(full_input_bytes)} bytes) ë¡œë“œ ì™„ë£Œ.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ '{input_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    feed_dict = {}
    current_offset = 0
    print("\nğŸ”ª ì…ë ¥ ë°ì´í„° ë¶„í•  ë° í…ì„œ ìƒì„±:")
    for info in model_inputs:
        chunk_bytes = full_input_bytes[current_offset : current_offset + info['size_bytes']]
        
        if len(chunk_bytes) < info['size_bytes']:
            print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ í¬ê¸°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. '{info['name']}' í…ì„œ ì²˜ë¦¬ ì¤‘ë‹¨.")
            return

        # ë°”ì´íŠ¸ ì²­í¬ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  reshape
        tensor = np.frombuffer(chunk_bytes, dtype=info['dtype']).reshape(info['shape'])
        feed_dict[info['name']] = tensor
        current_offset += info['size_bytes']
        print(f"  - '{info['name']}' í…ì„œ ìƒì„± ì™„ë£Œ (shape: {tensor.shape})")

    if current_offset != len(full_input_bytes):
        print(f"âš ï¸ ê²½ê³ : ì…ë ¥ íŒŒì¼ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ {len(full_input_bytes) - current_offset} bytes ë‚¨ì•˜ìŠµë‹ˆë‹¤.")

    # 4. ì¶”ë¡  ì‹¤í–‰
    print("\nğŸš€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    outputs = session.run(None, feed_dict)
    print("âœ… ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 5. ì²« ë²ˆì§¸ ì¶œë ¥ ê²°ê³¼ë¥¼ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ íŒŒì¼ì— ì €ì¥
    #   (ì°¸ê³ : ëª¨ë¸ ì¶œë ¥ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, í•„ìš”ì— ë”°ë¼ outputs[1], outputs[2] ë“±ì„ ì²˜ë¦¬í•´ì•¼ í•¨)
    output_tensor = outputs[0]
    print(f"   - ì¶œë ¥ í…ì„œ(0) ì •ë³´: Shape={output_tensor.shape}, Type={output_tensor.dtype}")
    try:
        output_tensor.tofile(output_path)
        print(f"\nğŸ’¾ ì¶”ë¡  ê²°ê³¼(ì²« ë²ˆì§¸ ì¶œë ¥)ê°€ '{os.path.basename(output_path)}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ì¶œë ¥ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ë‹¤ì¤‘ ì…ë ¥ì„ ì§€ì›í•˜ëŠ” ONNX ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸. ì…ë ¥ì„ ìë™ ë¶„ì„í•˜ê³  ë‹¨ì¼ bin íŒŒì¼ì„ ë¶„í• í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('-m', '--model', type=str, required=True, help="ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ.")
    parser.add_argument('-i', '--input', type=str, required=True, help="ëª¨ë“  ì…ë ¥ ë°ì´í„°ê°€ ìˆœì„œëŒ€ë¡œ í•©ì³ì§„ ë‹¨ì¼ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ê²½ë¡œ.")
    parser.add_argument('-o', '--output', type=str, required=True, help="ê²°ê³¼ë¥¼ ì €ì¥í•  ë°”ì´ë„ˆë¦¬ íŒŒì¼ ê²½ë¡œ.")
    parser.add_argument('--batch_size', type=int, default=1, help="ëª¨ë¸ì˜ ë™ì  ì…ë ¥ ì°¨ì›(ë°°ì¹˜ í¬ê¸°)ì„ ì§€ì •í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’: 1)")

    args = parser.parse_args()
    run_multi_input_inference(args.model, args.input, args.output, args.batch_size)